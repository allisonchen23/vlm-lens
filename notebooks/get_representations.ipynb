{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9bb1f741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import sqlite3\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, '../src')\n",
    "sys.path.insert(1, '..')\n",
    "\n",
    "from main import get_model\n",
    "from models.config import Config, IMAGE_TOKEN_IDS\n",
    "import db_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadd8408",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['notebooks/get_representations.ipynb',\n",
    "            '--config', '../configs/models/qwen/Qwen2-VL-2B-Instruct-VisualFlow.yaml']\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9779e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  8.07it/s]\n"
     ]
    }
   ],
   "source": [
    "model = get_model(config.architecture, config)\n",
    "# print(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cffc847d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2VLForConditionalGeneration(\n",
      "  (visual): Qwen2VisionTransformerPretrainedModel(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
      "    )\n",
      "    (rotary_pos_emb): VisionRotaryEmbedding()\n",
      "    (blocks): ModuleList(\n",
      "      (0-31): 32 x Qwen2VLVisionBlock(\n",
      "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): VisionSdpaAttention(\n",
      "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
      "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        )\n",
      "        (mlp): VisionMlp(\n",
      "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "          (act): QuickGELUActivation()\n",
      "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (merger): PatchMerger(\n",
      "      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=5120, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (model): Qwen2VLModel(\n",
      "    (embed_tokens): Embedding(151936, 1536)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen2VLDecoderLayer(\n",
      "        (self_attn): Qwen2VLSdpaAttention(\n",
      "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "          (rotary_emb): Qwen2VLRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2VLRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb16daf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running forward hooks on data: 100%|██████████| 2/2 [00:00<00:00,  2.91it/s]\n"
     ]
    }
   ],
   "source": [
    "model.run(save_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "affc9454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../output/Qwen2-VL-2B-Instruct-VisualFlow.db\n"
     ]
    }
   ],
   "source": [
    "db_path = model.config.output_db\n",
    "print(db_path)\n",
    "connection = sqlite3.connect(db_path)\n",
    "connection.row_factory = sqlite3.Row\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0635d2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_blue.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'input_ids', 'pooling_method': None, 'tensor_dim': 1, 'tensor_shape': 'torch.Size([1, 129])', 'tensor_bytes': 2204}\n",
      "{'id': 2, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_blue.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'visual.blocks.31', 'pooling_method': None, 'tensor_dim': 1280, 'tensor_shape': 'torch.Size([392, 1280])', 'tensor_bytes': 1004700}\n",
      "{'id': 3, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_blue.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'visual.merger', 'pooling_method': None, 'tensor_dim': 1536, 'tensor_shape': 'torch.Size([98, 1536])', 'tensor_bytes': 302236}\n",
      "{'id': 4, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_blue.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'model.layers.27.post_attention_layernorm', 'pooling_method': None, 'tensor_dim': 1536, 'tensor_shape': 'torch.Size([1, 129, 1536])', 'tensor_bytes': 397468}\n",
      "{'id': 5, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_blue.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'model.norm', 'pooling_method': None, 'tensor_dim': 1536, 'tensor_shape': 'torch.Size([1, 129, 1536])', 'tensor_bytes': 397468}\n",
      "{'id': 6, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_blue.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'lm_head', 'pooling_method': None, 'tensor_dim': 151936, 'tensor_shape': 'torch.Size([1, 129, 151936])', 'tensor_bytes': 39200668}\n",
      "{'id': 7, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_black.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'input_ids', 'pooling_method': None, 'tensor_dim': 1, 'tensor_shape': 'torch.Size([1, 129])', 'tensor_bytes': 2204}\n",
      "{'id': 8, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_black.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'visual.blocks.31', 'pooling_method': None, 'tensor_dim': 1280, 'tensor_shape': 'torch.Size([392, 1280])', 'tensor_bytes': 1004700}\n",
      "{'id': 9, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_black.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'visual.merger', 'pooling_method': None, 'tensor_dim': 1536, 'tensor_shape': 'torch.Size([98, 1536])', 'tensor_bytes': 302236}\n",
      "{'id': 10, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_black.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'model.layers.27.post_attention_layernorm', 'pooling_method': None, 'tensor_dim': 1536, 'tensor_shape': 'torch.Size([1, 129, 1536])', 'tensor_bytes': 397468}\n",
      "{'id': 11, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_black.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'model.norm', 'pooling_method': None, 'tensor_dim': 1536, 'tensor_shape': 'torch.Size([1, 129, 1536])', 'tensor_bytes': 397468}\n",
      "{'id': 12, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_black.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'lm_head', 'pooling_method': None, 'tensor_dim': 151936, 'tensor_shape': 'torch.Size([1, 129, 151936])', 'tensor_bytes': 39200668}\n"
     ]
    }
   ],
   "source": [
    "for r in cursor.execute(\"\"\"\n",
    "  SELECT id, image_id, name, image_path, prompt, label, layer, pooling_method, tensor_dim, tensor_shape,\n",
    "         length(tensor) AS tensor_bytes\n",
    "  FROM tensors\n",
    "  LIMIT 20\n",
    "\"\"\"):\n",
    "    print(dict(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "52ba5eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 3, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'label': None, 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_blue.png', 'layer': 'visual.merger', 'tensor_dim': 1536, 'tensor_shape': 'torch.Size([98, 1536])', 'tensor_bytes': 302236}\n",
      "{'id': 9, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'label': None, 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_black.png', 'layer': 'visual.merger', 'tensor_dim': 1536, 'tensor_shape': 'torch.Size([98, 1536])', 'tensor_bytes': 302236}\n"
     ]
    }
   ],
   "source": [
    "for r in cursor.execute(\"\"\"\n",
    "  SELECT id, name, label, image_path, layer, tensor_dim, tensor_shape,\n",
    "         length(tensor) AS tensor_bytes\n",
    "  FROM tensors\n",
    "  WHERE layer = \"visual.merger\"\n",
    "  LIMIT 20\n",
    "\"\"\"):\n",
    "    print(dict(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00975a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[151644   8948    198   2610    525    264  10950  17847     13 151645\n",
      "     198 151644    872    198 151652 151655 151655 151655 151655 151655\n",
      "  151655 151655 151655 151655 151655 151655 151655 151655 151655 151655\n",
      "  151655 151655 151655 151655 151655 151655 151655 151655 151655 151655\n",
      "  151655 151655 151655 151655 151655 151655 151655 151655 151655 151655\n",
      "  151655 151655 151655 151655 151655 151655 151655 151655 151655 151655\n",
      "  151655 151655 151655 151655 151655 151655 151655 151655 151655 151655\n",
      "  151655 151655 151655 151655 151655 151655 151655 151655 151655 151655\n",
      "  151655 151655 151655 151655 151655 151655 151655 151655 151655 151655\n",
      "  151655 151655 151655 151655 151655 151655 151655 151655 151655 151655\n",
      "  151655 151655 151655 151655 151655 151655 151655 151655 151655 151655\n",
      "  151655 151655 151655 151653  74785    279   1894    304    419   2168\n",
      "     304    825   3409     13 151645    198 151644  77091    198]\n",
      " [151644   8948    198   2610    525    264  10950  17847     13 151645\n",
      "     198 151644    872    198 151652 151655 151655 151655 151655 151655\n",
      "  151655 151655 151655 151655 151655 151655 151655 151655 151655 151655\n",
      "  151655 151655 151655 151655 151655 151655 151655 151655 151655 151655\n",
      "  151655 151655 151655 151655 151655 151655 151655 151655 151655 151655\n",
      "  151655 151655 151655 151655 151655 151655 151655 151655 151655 151655\n",
      "  151655 151655 151655 151655 151655 151655 151655 151655 151655 151655\n",
      "  151655 151655 151655 151655 151655 151655 151655 151655 151655 151655\n",
      "  151655 151655 151655 151655 151655 151655 151655 151655 151655 151655\n",
      "  151655 151655 151655 151655 151655 151655 151655 151655 151655 151655\n",
      "  151655 151655 151655 151655 151655 151655 151655 151655 151655 151655\n",
      "  151655 151655 151655 151653  74785    279   1894    304    419   2168\n",
      "     304    825   3409     13 151645    198 151644  77091    198]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.9883939244138062)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = db_utils.get_embeddings_by_layer(\n",
    "    db_path=db_path,\n",
    "    layer_name=\"input_ids\",\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "post_llm_embeddings = db_utils.get_embeddings_by_layer(\n",
    "    db_path=db_path,\n",
    "    layer_name=\"model.norm\",\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "\n",
    "input_ids = np.squeeze(np.array(tuple(map(list, zip(*input_ids)))[1])) # B x T\n",
    "print(input_ids)\n",
    "# print(torch.tensor(np.array(input_ids)).shape)\n",
    "\n",
    "post_llm_embeddings = np.squeeze(np.array(tuple(map(list, zip(*post_llm_embeddings)))[1])) # B x T x D\n",
    "# print(torch.tensor(np.array(post_llm_embeddings)).shape)\n",
    "\n",
    "\n",
    "\n",
    "visual_embs, n_visual_embs = db_utils.extract_visual_embeddings(\n",
    "    input_ids=input_ids,\n",
    "    llm_embeddings=post_llm_embeddings,\n",
    "    image_token_id=IMAGE_TOKEN_IDS['qwen'])\n",
    "\n",
    "mean_embs = db_utils.compute_mean_embeddings(\n",
    "    embeddings=visual_embs,\n",
    "    n_embeddings=n_visual_embs)\n",
    "\n",
    "db_utils.cosine_similarity_numpy(mean_embs, mean_embs)\n",
    "\n",
    "# TODO: for each unique layer in DB (or is this saved in model.config?), get the embedding representations\n",
    "# TODO: Create similarity matrix for each pair of layers (averaged over (B * (B-1) / 2)), and save a visual representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d587b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm-lens-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
