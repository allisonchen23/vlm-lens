{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bb1f741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import sqlite3\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, '../src')\n",
    "sys.path.insert(1, '..')\n",
    "\n",
    "from main import get_model\n",
    "from models.config import Config\n",
    "import db_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fadd8408",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['notebooks/get_representations.ipynb',\n",
    "            '--config', '../configs/models/qwen/Qwen2-VL-2B-Instruct-VisualFlow.yaml']\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9779e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.29it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "model = get_model(config.architecture, config)\n",
    "# print(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cffc847d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2VLForConditionalGeneration(\n",
      "  (visual): Qwen2VisionTransformerPretrainedModel(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
      "    )\n",
      "    (rotary_pos_emb): VisionRotaryEmbedding()\n",
      "    (blocks): ModuleList(\n",
      "      (0-31): 32 x Qwen2VLVisionBlock(\n",
      "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): VisionSdpaAttention(\n",
      "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
      "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        )\n",
      "        (mlp): VisionMlp(\n",
      "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "          (act): QuickGELUActivation()\n",
      "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (merger): PatchMerger(\n",
      "      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=5120, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (model): Qwen2VLModel(\n",
      "    (embed_tokens): Embedding(151936, 1536)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen2VLDecoderLayer(\n",
      "        (self_attn): Qwen2VLSdpaAttention(\n",
      "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "          (rotary_emb): Qwen2VLRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2VLRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb16daf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running forward hooks on data:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
      "         151645,    198, 151644,    872,    198, 151652, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151653,  74785,    279,   1894,\n",
      "            304,    419,   2168,    304,    825,   3409,     13, 151645,    198,\n",
      "         151644,  77091,    198]])\n",
      "torch.Size([1, 129])\n",
      "tensor(98)\n",
      "../data/test-images/black_in_blue.png\n",
      "../data/test-images/black_in_blue.png\n",
      "../data/test-images/black_in_blue.png\n",
      "../data/test-images/black_in_blue.png\n",
      "../data/test-images/black_in_blue.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running forward hooks on data:  50%|█████     | 1/2 [00:00<00:00,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
      "         151645,    198, 151644,    872,    198, 151652, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "         151655, 151655, 151655, 151655, 151655, 151653,  74785,    279,   1894,\n",
      "            304,    419,   2168,    304,    825,   3409,     13, 151645,    198,\n",
      "         151644,  77091,    198]])\n",
      "torch.Size([1, 129])\n",
      "tensor(98)\n",
      "../data/test-images/black_in_black.png\n",
      "../data/test-images/black_in_black.png\n",
      "../data/test-images/black_in_black.png\n",
      "../data/test-images/black_in_black.png\n",
      "../data/test-images/black_in_black.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running forward hooks on data: 100%|██████████| 2/2 [00:00<00:00,  2.68it/s]\n"
     ]
    }
   ],
   "source": [
    "model.run(save_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "affc9454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../output/Qwen2-VL-2B-Instruct-VisualFlow.db\n"
     ]
    }
   ],
   "source": [
    "db_path = model.config.output_db\n",
    "print(db_path)\n",
    "connection = sqlite3.connect(db_path)\n",
    "connection.row_factory = sqlite3.Row\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0635d2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '../data/test-images/black_in_blue.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'input_ids', 'pooling_method': None, 'tensor_dim': 1, 'tensor_shape': 'torch.Size([1, 129])', 'tensor_bytes': 2204}\n",
      "{'id': 2, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_blue.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'visual.blocks.31', 'pooling_method': None, 'tensor_dim': 1280, 'tensor_shape': 'torch.Size([392, 1280])', 'tensor_bytes': 1004700}\n",
      "{'id': 3, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_blue.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'visual.merger', 'pooling_method': None, 'tensor_dim': 1536, 'tensor_shape': 'torch.Size([98, 1536])', 'tensor_bytes': 302236}\n",
      "{'id': 4, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_blue.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'model.layers.27.post_attention_layernorm', 'pooling_method': None, 'tensor_dim': 1536, 'tensor_shape': 'torch.Size([1, 129, 1536])', 'tensor_bytes': 397468}\n",
      "{'id': 5, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_blue.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'model.norm', 'pooling_method': None, 'tensor_dim': 1536, 'tensor_shape': 'torch.Size([1, 129, 1536])', 'tensor_bytes': 397468}\n",
      "{'id': 6, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_blue.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'lm_head', 'pooling_method': None, 'tensor_dim': 151936, 'tensor_shape': 'torch.Size([1, 129, 151936])', 'tensor_bytes': 39200668}\n",
      "{'id': 7, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '../data/test-images/black_in_black.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'input_ids', 'pooling_method': None, 'tensor_dim': 1, 'tensor_shape': 'torch.Size([1, 129])', 'tensor_bytes': 2204}\n",
      "{'id': 8, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_black.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'visual.blocks.31', 'pooling_method': None, 'tensor_dim': 1280, 'tensor_shape': 'torch.Size([392, 1280])', 'tensor_bytes': 1004700}\n",
      "{'id': 9, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_black.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'visual.merger', 'pooling_method': None, 'tensor_dim': 1536, 'tensor_shape': 'torch.Size([98, 1536])', 'tensor_bytes': 302236}\n",
      "{'id': 10, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_black.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'model.layers.27.post_attention_layernorm', 'pooling_method': None, 'tensor_dim': 1536, 'tensor_shape': 'torch.Size([1, 129, 1536])', 'tensor_bytes': 397468}\n",
      "{'id': 11, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_black.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'model.norm', 'pooling_method': None, 'tensor_dim': 1536, 'tensor_shape': 'torch.Size([1, 129, 1536])', 'tensor_bytes': 397468}\n",
      "{'id': 12, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_black.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'lm_head', 'pooling_method': None, 'tensor_dim': 151936, 'tensor_shape': 'torch.Size([1, 129, 151936])', 'tensor_bytes': 39200668}\n",
      "{'id': 13, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_blue.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'input_ids', 'pooling_method': None, 'tensor_dim': 1, 'tensor_shape': 'torch.Size([1, 129])', 'tensor_bytes': 2204}\n",
      "{'id': 14, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_blue.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'visual.blocks.31', 'pooling_method': None, 'tensor_dim': 1280, 'tensor_shape': 'torch.Size([392, 1280])', 'tensor_bytes': 1004700}\n",
      "{'id': 15, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_blue.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'visual.merger', 'pooling_method': None, 'tensor_dim': 1536, 'tensor_shape': 'torch.Size([98, 1536])', 'tensor_bytes': 302236}\n",
      "{'id': 16, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_blue.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'model.layers.27.post_attention_layernorm', 'pooling_method': None, 'tensor_dim': 1536, 'tensor_shape': 'torch.Size([1, 129, 1536])', 'tensor_bytes': 397468}\n",
      "{'id': 17, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_blue.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'model.norm', 'pooling_method': None, 'tensor_dim': 1536, 'tensor_shape': 'torch.Size([1, 129, 1536])', 'tensor_bytes': 397468}\n",
      "{'id': 18, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_blue.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'lm_head', 'pooling_method': None, 'tensor_dim': 151936, 'tensor_shape': 'torch.Size([1, 129, 151936])', 'tensor_bytes': 39200668}\n",
      "{'id': 19, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_black.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'input_ids', 'pooling_method': None, 'tensor_dim': 1, 'tensor_shape': 'torch.Size([1, 129])', 'tensor_bytes': 2204}\n",
      "{'id': 20, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_black.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'visual.blocks.31', 'pooling_method': None, 'tensor_dim': 1280, 'tensor_shape': 'torch.Size([392, 1280])', 'tensor_bytes': 1004700}\n"
     ]
    }
   ],
   "source": [
    "for r in cursor.execute(\"\"\"\n",
    "  SELECT id, image_id, name, image_path, prompt, label, layer, pooling_method, tensor_dim, tensor_shape,\n",
    "         length(tensor) AS tensor_bytes\n",
    "  FROM tensors\n",
    "  LIMIT 20\n",
    "\"\"\"):\n",
    "    print(dict(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ba5eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '../data/test-images/black_in_blue.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'input_ids', 'pooling_method': None, 'tensor_dim': 1, 'tensor_shape': 'torch.Size([1, 129])', 'tensor_bytes': 2204}\n",
      "{'id': 7, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '../data/test-images/black_in_black.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'input_ids', 'pooling_method': None, 'tensor_dim': 1, 'tensor_shape': 'torch.Size([1, 129])', 'tensor_bytes': 2204}\n",
      "{'id': 13, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_blue.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'input_ids', 'pooling_method': None, 'tensor_dim': 1, 'tensor_shape': 'torch.Size([1, 129])', 'tensor_bytes': 2204}\n",
      "{'id': 19, 'image_id': None, 'name': 'Qwen/Qwen2-VL-2B-Instruct', 'image_path': '/n/fs/ac-aiexhibit/vlm-lens/data/test-images/black_in_black.png', 'prompt': 'Describe the color in this image in one word.', 'label': None, 'layer': 'input_ids', 'pooling_method': None, 'tensor_dim': 1, 'tensor_shape': 'torch.Size([1, 129])', 'tensor_bytes': 2204}\n"
     ]
    }
   ],
   "source": [
    "for r in cursor.execute(\"\"\"\n",
    "  SELECT id, name, image_path, layer, tensor_dim, tensor_shape,\n",
    "         length(tensor) AS tensor_bytes\n",
    "  FROM tensors\n",
    "  WHERE layer = \"input_ids\"\n",
    "  LIMIT 20\n",
    "\"\"\"):\n",
    "    print(dict(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d00975a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "('input_ids', array([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,\n",
      "            13, 151645,    198, 151644,    872,    198, 151652, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655,\n",
      "        151655, 151653,  74785,    279,   1894,    304,    419,   2168,\n",
      "           304,    825,   3409,     13, 151645,    198, 151644,  77091,\n",
      "           198]]), None)\n",
      "torch.Size([4, 1, 129])\n"
     ]
    }
   ],
   "source": [
    "embeddings = db_utils.get_embeddings_by_layer(\n",
    "    db_path=db_path,\n",
    "    layer_name=\"input_ids\",\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "print(type(embeddings[0]))\n",
    "print(embeddings[0])\n",
    "embeddings = tuple(map(list, zip(*embeddings)))[1]\n",
    "\n",
    "print(torch.tensor(np.array(embeddings)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d587b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm-lens-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
