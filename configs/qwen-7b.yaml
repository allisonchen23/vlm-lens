architecture: clip
model_path: openai/clip-vit-base-patch32
model:
  - torch_dtype: auto
output_db: output/clip.db
dataset:
  - dataset_path: halasheta/coco_mapped
  - dataset_split: train
  # - local_dataset_path: data/COCO/mapped
  # - dataset_split: val2017
  # - image_dataset_path: data/COCO/test
  # - image_split: train
modules:
  - text_projection
