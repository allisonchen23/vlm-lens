architecture: clip
model_path: openai/clip-vit-base-patch32
model:
  - torch_dtype: auto
output_db: output/hosted-imgs-clip.db
dataset:
  # hosted, images
  - dataset_path: halasheta/coco_images
  - dataset_split: train

  # hosted, image paths
  # - dataset_path: halasheta/coco_mapped
  # - dataset_split: train

  # - local_dataset_path: data/COCO/mapped
  # - dataset_split: val2017
  # - image_dataset_path: data/COCO/test
  # - image_split: train
modules:
  - text_projection
