architecture: clip
model_path: openai/clip-vit-base-patch32
model:
  - torch_dtype: auto
dataset:
  - text_dataset_path: wonderwind271/CLEVR_val_categories
  - image_dataset_path: /path/to/CLEVR_v1.0/images
  - text_split: color
  - image_split: val
  - prompt_column: Q
  - answer_column: A
  - image_column: image_filename
output_db: clip.db
modules:
  - vision_model.encoder
