{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1a514cc",
   "metadata": {},
   "source": [
    "### VLM-Lens: Probeing Vision-Language Models with Lens\n",
    "\n",
    "This notebook demonstrates how to use VLM-Lens to probe vision-language models (VLMs) on a pre-defined dataset. Here, we use the CLEVR-based dataset to illustrate the process. \n",
    "\n",
    "We use the `boolean` split of `compling/CLEVR_categories` dataset, which can be found on [Hugging Face Datasets](https://huggingface.co/datasets/compling/CLEVR_categories/viewer/default/boolean).\n",
    "The dataset contains images and corresponding questions about the image, and asks whether the question is true or false based on the image content.\n",
    "\n",
    "Qwen-2B will be used as the example model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aaab57",
   "metadata": {},
   "source": [
    "### Step 0: Environmental Setup\n",
    "\n",
    "As mentioned in the [minimal guide](https://github.com/compling-wat/vlm-lens/blob/main/tutorial-notebooks/guide.ipynb), we first need to install the required packages and import necessary modules.\n",
    "\n",
    "There are a few more dependencies needed for this notebook, in addition to base environment setup. Under a virtual environment, we can install them via pip:\n",
    "```bash\n",
    "pip install -r envs/probe/requirements.txt\n",
    "```\n",
    "\n",
    "### Step 1: Extract Features \n",
    "\n",
    "Feature extraction is the core functionality of VLM-Lens. It allows you to extract features from images and text using pre-trained models.\n",
    "To extract the LLAVA features, we can run the following code (note - this may take 20-30 minutes depending on your hardware):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a469b3ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! python -m src.main --config configs/models/qwen/qwen-2b-clevr.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319c5898",
   "metadata": {},
   "source": [
    "The output features will be saved in `output/qwen-boolean.db` by default. You can change the output path in the config file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cc30ea",
   "metadata": {},
   "source": [
    "### Step 2: Run the Probe Training Script\n",
    "\n",
    "After having the features extracted, we can run the following probe training script to train a probe model on the extracted features. The probe model will be trained to predict the answer to the question based on the image features. \n",
    "\n",
    "This make take ~30 minutes depending on your hardware. It will be faster if you have a customized dataset with fewer samples.\n",
    "\n",
    "It's worth noting that the current probe training script assumes that there is no explicit splits between training and testing data; therefore, it automatically splits the data into training and testing sets. You can modify the script to use your own splits if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eae6f0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! python -m src.probe.main --config configs/probe/qwen/clevr-boolean-l13-example.yaml  # run with --debug if you'd like to see more logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad72778c",
   "metadata": {},
   "source": [
    "Let's take a closer look at the YAML config file used in the above command: `configs/probe/qwen/clevr-boolean-l13-example.yaml`.\n",
    "\n",
    "```yaml\n",
    "model:\n",
    "  - activation: ReLU\n",
    "  - hidden_size: 512\n",
    "  - num_layers: 2\n",
    "  - save_dir: output/qwen_boolean_probe_l13\n",
    "training:\n",
    "  - batch_size: [64, 128]\n",
    "  - num_epochs: [50]\n",
    "  - learning_rate: [0.001]\n",
    "  - optimizer: AdamW\n",
    "  - loss: CrossEntropyLoss\n",
    "test:\n",
    "  - batch_size: 32\n",
    "  - loss: CrossEntropyLoss\n",
    "data:\n",
    "  - input_db: output/qwen-boolean.db\n",
    "  - db_name: tensors\n",
    "  - input_layer: model.layers.13.post_attention_layernorm\n",
    "```\n",
    "\n",
    "The config file specifies the MLP non-linear activation function (ReLU), training parameters (hidden size = 512, 2 layers), and data source. \n",
    "\n",
    "The probe trainer will do a grid search over the training parameters (batch size, number of epochs, learning rate) to find the best combination. It will select the best parameter combination based on the cross-validation loss, and then evaluate the model on the test set.\n",
    "\n",
    "You can modify these parameters to suit your needs.\n",
    "\n",
    "The final probing results will be saved in `output/qwen_boolean_probe_l13/`, as specified in the config file. Under this directory, you will find: \n",
    "- `probe.pth`: the trained probe model\n",
    "- `probe_data.json`: the probing results, including training and testing predictions, accuracy, loss, and best hyperparameters, as well as the p-value that the probe model is better than random guessing (labeled by \"shuffle\" in the probe results)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
